{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emergency_Response_Incidents.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Emergency_Response_Incidents:\n",
        "\n",
        "a. To get the required columns, use this module: \n",
        "\n",
        "\n",
        "1.   get_area_of_interest(df_spark, interested_columns)\n",
        "\n",
        "\n",
        "b. Preprocessing pipeline: Pass your data through these functions. (if your columns fall in those categories)\n",
        "\n",
        "1.   valid_date_check(date)\n",
        "2.   valid_time_check(time)\n",
        "3.   reverse_geo_code_boros(df_spark, Latitude, Longitude, Boro, lat_index, long_index)\n",
        "4.   refine_age_group_race(df_spark, victim_age_group=None, suspect_age_group=None, suspect_race=None, victim_race=None)\n",
        "5.   refine_sex_gender_impute(df_spark, suspect_age=None, suspect_gender=None, victim_age=None, victim_gender=None)\n",
        "6.   refine_precinct_jur(df_spark, precinct=None, Jur_code=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "EtwLWOuvVE1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8dUqIINUs-T",
        "outputId": "3c7090ae-1cb4-488e-d8dc-d33e512a9960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 39 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 49.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=f0e25d3257e061dfa9f95f45fed41290050c9161f697c7fe84663e7724ebdfad\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
            "Collecting openclean\n",
            "  Downloading openclean-0.2.1-py3-none-any.whl (5.2 kB)\n",
            "Collecting openclean-core==0.4.1\n",
            "  Downloading openclean_core-0.4.1-py3-none-any.whl (267 kB)\n",
            "\u001b[K     |████████████████████████████████| 267 kB 35.5 MB/s \n",
            "\u001b[?25hCollecting jsonschema>=3.2.0\n",
            "  Downloading jsonschema-4.2.1-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (0.3.4)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (1.1.5)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (1.4.4)\n",
            "Collecting histore>=0.4.0\n",
            "  Downloading histore-0.4.1-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 57.3 MB/s \n",
            "\u001b[?25hCollecting jellyfish\n",
            "  Downloading jellyfish-0.8.9.tar.gz (137 kB)\n",
            "\u001b[K     |████████████████████████████████| 137 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (2.23.0)\n",
            "Collecting refdata>=0.2.0\n",
            "  Downloading refdata-0.2.0-py3-none-any.whl (37 kB)\n",
            "Collecting flowserv-core>=0.8.0\n",
            "  Downloading flowserv_core-0.9.2-py3-none-any.whl (260 kB)\n",
            "\u001b[K     |████████████████████████████████| 260 kB 64.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (2.8.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 70.6 MB/s \n",
            "\u001b[?25hCollecting passlib\n",
            "  Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
            "\u001b[K     |████████████████████████████████| 525 kB 71.4 MB/s \n",
            "\u001b[?25hCollecting gitpython\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy>=1.3.18 in /usr/local/lib/python3.7/dist-packages (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (1.4.27)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.7/dist-packages (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (7.1.2)\n",
            "Collecting paramiko\n",
            "  Downloading paramiko-2.8.1-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 71.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml-include\n",
            "  Downloading pyyaml_include-1.2.post2-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from histore>=0.4.0->openclean-core==0.4.1->openclean) (5.4.8)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (0.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (4.8.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (5.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (21.2.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->openclean-core==0.4.1->openclean) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->openclean-core==0.4.1->openclean) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->openclean-core==0.4.1->openclean) (1.15.0)\n",
            "Collecting tableprint\n",
            "  Downloading tableprint-0.9.1-py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: pooch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from refdata>=0.2.0->openclean-core==0.4.1->openclean) (1.5.2)\n",
            "Collecting datasize>=1.0.0\n",
            "  Downloading datasize-1.0.0.tar.gz (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 55.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.3.0->refdata>=0.2.0->openclean-core==0.4.1->openclean) (21.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy>=1.3.18->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (1.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.3.0->refdata>=0.2.0->openclean-core==0.4.1->openclean) (3.0.6)\n",
            "Collecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
            "\u001b[K     |████████████████████████████████| 961 kB 34.5 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 46.2 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (2.21)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 37.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->openclean-core==0.4.1->openclean) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->openclean-core==0.4.1->openclean) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->openclean-core==0.4.1->openclean) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->openclean-core==0.4.1->openclean) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->openclean-core==0.4.1->openclean) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->openclean-core==0.4.1->openclean) (3.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from tableprint->refdata>=0.2.0->openclean-core==0.4.1->openclean) (0.2.5)\n",
            "Building wheels for collected packages: datasize, jellyfish\n",
            "  Building wheel for datasize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datasize: filename=datasize-1.0.0-py2.py3-none-any.whl size=155047 sha256=41db6e897bd659a516bfde6bb0eb1237f04716be848189c22d4dd4c307ba0eb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/b5/32/d8836896da6aca7f9c5748670ea6110d1385c262bf3abcca30\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.8.9-cp37-cp37m-linux_x86_64.whl size=73220 sha256=0f55fb3650bc7ba75d19a75371b4f7de59569806555be41096ac5ca0be26f3a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/a9/ef/5d8742e72deaf0d1de327a180d008c2c0299367581800ea73f\n",
            "Successfully built datasize jellyfish\n",
            "Installing collected packages: smmap, pyyaml, pynacl, gitdb, cryptography, bcrypt, tableprint, pyyaml-include, passlib, paramiko, jsonschema, gitpython, datasize, refdata, jellyfish, histore, flowserv-core, openclean-core, openclean\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.5.9 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed bcrypt-3.2.0 cryptography-36.0.0 datasize-1.0.0 flowserv-core-0.9.2 gitdb-4.0.9 gitpython-3.1.24 histore-0.4.1 jellyfish-0.8.9 jsonschema-4.2.1 openclean-0.2.1 openclean-core-0.4.1 paramiko-2.8.1 passlib-1.7.4 pynacl-1.4.0 pyyaml-5.4.1 pyyaml-include-1.2.post2 refdata-0.2.0 smmap-5.0.0 tableprint-0.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install openclean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing packages required\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import os\n",
        "import requests\n",
        "from six.moves import urllib\n",
        "import sys \n",
        "import pandas as pd\n",
        "import matplotlib \n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import IPython\n",
        "from IPython import display\n",
        "import sklearn\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from openclean.pipeline import stream\n",
        "from openclean.profiling.column import DefaultColumnProfiler\n",
        "from openclean.data.source.socrata import Socrata\n",
        "from openclean.pipeline import stream\n",
        "from openclean.function.eval.datatype import IsDatetime\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import udf, struct\n",
        "from pyspark.sql.types import StringType"
      ],
      "metadata": {
        "id": "fOq89ZjQU0yv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import ArcGIS\n",
        "geocoder=ArcGIS()\n",
        "#example:\n",
        "geocoder.reverse('40.61157006600007, -73.74736517199995')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjam1thSU34J",
        "outputId": "24f6154c-a8e6-42d3-8eca-930b388331f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Location(11-64 Redfern Ave, Far Rockaway, New York 11691, USA, (40.61161616586613, -73.74738361194636, 0.0))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Spark Session\n",
        "sc = SparkContext.getOrCreate();\n",
        "spark = SparkSession(sc)"
      ],
      "metadata": {
        "id": "Jye4yIQdU7WB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading file from NYC Open Data\n",
        "#https://drive.google.com/uc?export=download&id=1aeFR66MAgDXobrEosENZhBlq0dAqHEQw\n",
        "#File uploaded on drive to make it available for all\n",
        "\n",
        "fn_src = 'https://drive.google.com/uc?export=download&id=1aeFR66MAgDXobrEosENZhBlq0dAqHEQw'\n",
        "fn_dst = '/content/Emergency_Response_Incidents.csv'\n",
        "\n",
        "from six.moves import urllib\n",
        "\n",
        "if os.path.isfile(fn_dst):\n",
        "    print('File has already been downloaded', fn_dst)\n",
        "else:\n",
        "    print('Fetching file. This may take a while...', fn_dst)\n",
        "    urllib.request.urlretrieve(fn_src, fn_dst)\n",
        "    print('File %s has been downloaded' % fn_dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkeHipz9U-Rj",
        "outputId": "891f3200-5663-47f2-be1a-ab1d7d53cb40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching file. This may take a while... /content/Emergency_Response_Incidents.csv\n",
            "File /content/Emergency_Response_Incidents.csv has been downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = 'https://data.beta.nyc/dataset/0ff93d2d-90ba-457c-9f7e-39e47bf2ac5f/resource/7caac650-d082-4aea-9f9b-3681d568e8a5/download/nyc_zip_borough_neighborhoods_pop.csv'\n",
        "dst = 'nyc_zip_borough_neighborhoods_pop.csv'\n",
        "\n",
        "#https://data.cityofnewyork.us/resource/h9gi-nx95.csv\n",
        "\n",
        "from six.moves import urllib\n",
        "\n",
        "if os.path.isfile(dst):\n",
        "    print('File %s has already been downloaded' % dst)\n",
        "else:\n",
        "    urllib.request.urlretrieve(src, dst)\n",
        "    print('File %s has been downloaded' % dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkp1LzrKWM99",
        "outputId": "4d9157ac-9868-4df6-c3b0-758bde402554"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File nyc_zip_borough_neighborhoods_pop.csv has been downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#similarly, lets get them into pyspark rdd\n",
        "def get_area_of_interest(df_spark, interested_columns):\n",
        "  df_spark=df_spark.select(interested_columns)\n",
        "  return df_spark"
      ],
      "metadata": {
        "id": "QSORcsBrVfcD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Module for date related columns\n",
        "\n",
        "As the dataset is for the data from 2006 to 2020, we can see that there is data from unknown format of \"1010-05-14\" to the year 2020. We need to clean this. Over here, we remove the null values where the complaint date is <2006. "
      ],
      "metadata": {
        "id": "up2fhHPMhWXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_date_check(date, format):\n",
        "  if date==None or date==\" \" or date==\"\":\n",
        "      return False\n",
        "  else:\n",
        "    date_cpy=date\n",
        "    split_date=date.split(\"-\")\n",
        "    format_date=format.split(\"-\")\n",
        "    if len(split_date)!=3 and len(format_date)!=3:\n",
        "      date=date.split(\"/\")\n",
        "      format=format.split(\"/\")\n",
        "    else:\n",
        "      date=split_date\n",
        "      format=format_date\n",
        "    if len(date)!=3:\n",
        "      return False\n",
        "    try:\n",
        "      year=int(date[format.index('yyyy')])\n",
        "      month=int(date[format.index('mm')])\n",
        "      day=int(date[format.index('dd')])\n",
        "      if year>=2006 and year<=2020:\n",
        "        try:\n",
        "          refined_date=datetime.datetime(year, month, day)\n",
        "          return True\n",
        "        except:\n",
        "          return False\n",
        "      else:\n",
        "        return False\n",
        "    except:\n",
        "      return False"
      ],
      "metadata": {
        "id": "kml5KmPEeAK4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Module for time related columns\n",
        "\n",
        "Similarly, lets check for the time as well. Here we must have time between \n",
        "the standard 24 hours."
      ],
      "metadata": {
        "id": "3OU8ZkkIhSf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Deleting invalid time\n",
        "def valid_time_check(time):\n",
        "  if time==None or time==\" \" or time==\"\":\n",
        "    return False\n",
        "  else :\n",
        "    cpy_time=time\n",
        "    time=time.split(\":\")\n",
        "    try:\n",
        "      hour=int(time[0])\n",
        "      mins=int(time[1])\n",
        "      secs= int(time[2])\n",
        "      # if hours is 24 then change it to 0 hours\n",
        "      if hour == 24 and mins== 0 and secs == 0:\n",
        "        hour=0\n",
        "      try:\n",
        "        newTime= datetime.time(hour,mins,secs)\n",
        "        return True\n",
        "      except :\n",
        "        return False\n",
        "    except:\n",
        "      return False"
      ],
      "metadata": {
        "id": "6LroG_OQeCe9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Module for Age Group and Race columns\n",
        "The module works for only those columns whose column names are passed"
      ],
      "metadata": {
        "id": "3gzYDuVwhO3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_age_group_race(df_spark, victim_age_group=None, suspect_age_group=None, suspect_race=None, victim_race=None):\n",
        "  #params: dataframe, col names for the respective age, gender cols\n",
        "  if victim_age_group:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[victim_age_group])\n",
        "  if suspect_age_group:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[suspect_age_group])\n",
        "  if suspect_race:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[suspect_race])\n",
        "  if victim_race:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[victim_race])\n",
        "  return df_spark"
      ],
      "metadata": {
        "id": "7r5AKpmteHSg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Module for Gender, Race Columns for suspects and victims\n",
        "\n",
        "The module works for only those columns whose column names are passed"
      ],
      "metadata": {
        "id": "69arM9hIhOIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_sex_gender_impute(df_spark, suspect_age=None, suspect_gender=None, victim_age=None, victim_gender=None):\n",
        "  #params: dataframe, col names for the respective age, gender cols\n",
        "  if suspect_age:\n",
        "    df_spark=df_spark.na.fill(\"U\",subset=[suspect_age])\n",
        "  if victim_age:\n",
        "    df_spark=df_spark.na.fill(\"U\",subset=[victim_age])\n",
        "  if suspect_gender:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[suspect_gender])\n",
        "  if victim_gender:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[victim_gender])\n",
        "  return df_spark"
      ],
      "metadata": {
        "id": "htQ2JRcseLNN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.a: Module for Precinct, Jurisdiction Code:\n",
        "  dropping the null values\n",
        "\n",
        "  The module works for only those columns whose column names are passed along with the df"
      ],
      "metadata": {
        "id": "d_NovxDghKwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_precinct_check(precinct):\n",
        "  if precinct==None or precinct==\" \" or precinct==\"\":\n",
        "    return False\n",
        "  else :\n",
        "    return True\n",
        "\n",
        "def valid_jur_check(jur):\n",
        "  if jur==None or jur==\" \" or jur==\"\":\n",
        "    return False\n",
        "  else :\n",
        "    return True"
      ],
      "metadata": {
        "id": "niQRHl7CeQR8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.b Module for Reverse Geocoding the boroughs using latitudes and longitudes.\n",
        "\n",
        "1. First we will remove the rows where latitude, longitude and boroughs are null. (around 450 tuples removed)\n",
        "2. Then, where the boroughs are empty, take the latitude and longitude value and reverse geocode it using the module \"reverseGeocoder\".\n",
        "3. Impute the borough name retrived in the empty space.\n",
        "\n",
        "\n",
        "### USING MASTER DATASET\n",
        "In the case of geocoding, geocoder gives us the zipcodes based on the latitude and longitude values. Inturn, we can use the master dataset of zipcodes inorder to retrive the borough names\n",
        "\n",
        "\n",
        "\n",
        "NOTE: The dataset can be downloaded from : https://data.beta.nyc/en/dataset/pediacities-nyc-neighborhoods/resource/7caac650-d082-4aea-9f9b-3681d568e8a5"
      ],
      "metadata": {
        "id": "YtxVasX_g8OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_geo_code_boros(df_spark, Latitude, Longitude, Boro, lat_index, long_index):\n",
        "  #select data where we have to impute\n",
        "  df_temp_boro_clean=df_spark.filter((df_spark[Latitude].isNotNull()) & (df_spark[Longitude].isNotNull()))\n",
        "  boro_cleaner=df_temp_boro_clean.filter((df_temp_boro_clean[Boro].isNull())|(df_temp_boro_clean[Boro]=='NEW YORK'))\n",
        "\n",
        "  # print(\"We have \"+ str(boro_cleaner.count())+ \" points to impute\")\n",
        "  print(\"___intializing Zip Code Look up ____\")\n",
        "  print(\"____ imputing the points ____\")\n",
        "\n",
        "\n",
        "  #use your path for master dataset here. \n",
        "  df_zips=pd.read_csv(dst)\n",
        "  zip_master={}\n",
        "  zips=df_zips['zip']\n",
        "  boro=df_zips['borough']\n",
        "  for i, j in zip(zips, boro):\n",
        "    zip_master[i]=j\n",
        "  zip_master[10020]='Manhattan'\n",
        "  zip_master[11249]='Brooklyn'\n",
        "\n",
        "  def reverseGeoCoder(latitude, longitude):\n",
        "    loc=geocoder.reverse(str(latitude)+', '+str(longitude), timeout=1000)\n",
        "    zipCode=str(loc).split(\",\")[2][-5:]\n",
        "    if not int(zipCode) in zip_master:\n",
        "      boro=\"UNKNOWN\"\n",
        "    else:\n",
        "      boro=zip_master[int(zipCode)]\n",
        "    boro=boro.upper()\n",
        "    return boro\n",
        "\n",
        "  #creating UD function\n",
        "  ud_func= udf(reverseGeoCoder, StringType())\n",
        "  boro_cleaned_dataframe = boro_cleaner.withColumn(Boro, ud_func(boro_cleaner[lat_index], boro_cleaner[long_index]))\n",
        "\n",
        "  #joining the imputed dataset to the maindataset and returning\n",
        "\n",
        "  joiner_dataset=df_spark.filter((df_spark[Latitude].isNotNull()) & (df_spark[Boro]!='NEW YORK') & (df_spark[Longitude].isNotNull()) & (df_spark[Boro].isNotNull()))\n",
        "  fin_df=joiner_dataset.union(boro_cleaned_dataframe)\n",
        "  return fin_df"
      ],
      "metadata": {
        "id": "tGGvupG0eQsb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NYPD Emergency_Response_Incidents 11085 Rows of data.\n",
        "\n",
        "Confidence Level:\t95% \n",
        "Confidence Interval: 5\n",
        "Population:\t\n",
        "11000\n",
        "      \n",
        "Sample size needed:\t\n",
        "377\n"
      ],
      "metadata": {
        "id": "aWqXQ6Xmgz70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark=spark.read.option(\"header\",True).csv(fn_dst,inferSchema=True)\n",
        "print(df_spark.count())\n",
        "df_spark=df_spark.sample(0.03290)\n",
        "df_spark.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq0_3ZPweYnA",
        "outputId": "a3616820-10db-484e-b2cd-1b5b89b5c91c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11085\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "351"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROFILING TO CHECK FOR NULL VALUES IN ALL THE COLUMNS"
      ],
      "metadata": {
        "id": "vQkebErhmlXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandasDF = df_spark.toPandas()\n",
        "ds=stream(pandasDF)\n",
        "#Creating profile of our dataset\n",
        "profiles = ds.profile(default_profiler=DefaultColumnProfiler)\n",
        "profiles.stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Y0ftSoh7mU0y",
        "outputId": "ac26a128-c0e5-4586-9e0c-95ce02fe7d30"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total</th>\n",
              "      <th>empty</th>\n",
              "      <th>distinct</th>\n",
              "      <th>uniqueness</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Incident Type</th>\n",
              "      <td>351</td>\n",
              "      <td>0</td>\n",
              "      <td>98</td>\n",
              "      <td>0.279202</td>\n",
              "      <td>5.724212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Location</th>\n",
              "      <td>351</td>\n",
              "      <td>27</td>\n",
              "      <td>311</td>\n",
              "      <td>0.959877</td>\n",
              "      <td>8.237321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Borough</th>\n",
              "      <td>351</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0.037037</td>\n",
              "      <td>2.432201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Creation Date</th>\n",
              "      <td>351</td>\n",
              "      <td>0</td>\n",
              "      <td>340</td>\n",
              "      <td>0.968661</td>\n",
              "      <td>8.382365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Closed Date</th>\n",
              "      <td>351</td>\n",
              "      <td>93</td>\n",
              "      <td>258</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.011227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latitude</th>\n",
              "      <td>351</td>\n",
              "      <td>53</td>\n",
              "      <td>277</td>\n",
              "      <td>0.929530</td>\n",
              "      <td>8.005247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Longitude</th>\n",
              "      <td>351</td>\n",
              "      <td>53</td>\n",
              "      <td>277</td>\n",
              "      <td>0.929530</td>\n",
              "      <td>8.005247</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               total  empty  distinct  uniqueness   entropy\n",
              "Incident Type    351      0        98    0.279202  5.724212\n",
              "Location         351     27       311    0.959877  8.237321\n",
              "Borough          351      0        13    0.037037  2.432201\n",
              "Creation Date    351      0       340    0.968661  8.382365\n",
              "Closed Date      351     93       258    1.000000  8.011227\n",
              "Latitude         351     53       277    0.929530  8.005247\n",
              "Longitude        351     53       277    0.929530  8.005247"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pandasDF.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f7wiqnybplIp",
        "outputId": "94a3babc-a2bd-453f-acfb-a8b9dea67921"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Incident Type</th>\n",
              "      <th>Location</th>\n",
              "      <th>Borough</th>\n",
              "      <th>Creation Date</th>\n",
              "      <th>Closed Date</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fire-2nd Alarm</td>\n",
              "      <td>238 East 24 Street</td>\n",
              "      <td>Manhattan</td>\n",
              "      <td>11/25/2016 04:06:09 AM</td>\n",
              "      <td>None</td>\n",
              "      <td>40.714422</td>\n",
              "      <td>-74.006076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Administration-Other</td>\n",
              "      <td>188 Street &amp; 75 Avenue</td>\n",
              "      <td>Queens</td>\n",
              "      <td>05/10/2017 01:36:12 PM</td>\n",
              "      <td>None</td>\n",
              "      <td>40.714004</td>\n",
              "      <td>-73.829989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Utility-Gas Low Pressure</td>\n",
              "      <td>Noble Avenue &amp; Watson Avenue</td>\n",
              "      <td>Bronx</td>\n",
              "      <td>01/30/2017 09:47:09 PM</td>\n",
              "      <td>None</td>\n",
              "      <td>40.827301</td>\n",
              "      <td>-73.869179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fire-2nd Alarm</td>\n",
              "      <td>447 Wales Avenue</td>\n",
              "      <td>Bronx</td>\n",
              "      <td>07/26/2017 03:06:44 AM</td>\n",
              "      <td>None</td>\n",
              "      <td>40.810281</td>\n",
              "      <td>-73.908206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fire-7th Alarm</td>\n",
              "      <td>110-14 Liberty Avenue</td>\n",
              "      <td>Queens</td>\n",
              "      <td>03/04/2017 11:13:52 PM</td>\n",
              "      <td>None</td>\n",
              "      <td>40.714004</td>\n",
              "      <td>-73.829989</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Incident Type                      Location  ...   Latitude  Longitude\n",
              "0            Fire-2nd Alarm            238 East 24 Street  ...  40.714422 -74.006076\n",
              "1      Administration-Other        188 Street & 75 Avenue  ...  40.714004 -73.829989\n",
              "2  Utility-Gas Low Pressure  Noble Avenue & Watson Avenue  ...  40.827301 -73.869179\n",
              "3            Fire-2nd Alarm              447 Wales Avenue  ...  40.810281 -73.908206\n",
              "4            Fire-7th Alarm         110-14 Liberty Avenue  ...  40.714004 -73.829989\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Select the columns that are common with the original dataset:\n",
        "1. Incident Type\n",
        "2. Borough\n",
        "3. Creation Date\n",
        "4. Latitude\n",
        "5. Longitude\n",
        "\n",
        "\n",
        "\n",
        "**We can consider the primary key along with this**\n",
        "**\"Incident Type\"**\n"
      ],
      "metadata": {
        "id": "paHwx8COfUAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interested_columns_1=['Incident Type', 'Location', 'Borough', 'Creation Date', 'Latitude', 'Longitude']\n",
        "df_spark=get_area_of_interest(df_spark, interested_columns_1)"
      ],
      "metadata": {
        "id": "Mk8tsxeLqdz_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.show()"
      ],
      "metadata": {
        "id": "GymGzo9SfMN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "955bdd24-2768-40ef-944c-ced6483fcae7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+--------------------+------------------+------------------+\n",
            "|       Incident Type|            Location|  Borough|       Creation Date|          Latitude|         Longitude|\n",
            "+--------------------+--------------------+---------+--------------------+------------------+------------------+\n",
            "|      Fire-2nd Alarm|  238 East 24 Street|Manhattan|11/25/2016 04:06:...| 40.71442154062271|-74.00607638041981|\n",
            "|Administration-Other|188 Street & 75 A...|   Queens|05/10/2017 01:36:...| 40.71400364095638|-73.82998933154158|\n",
            "|Utility-Gas Low P...|Noble Avenue & Wa...|    Bronx|01/30/2017 09:47:...| 40.82730091310776|-73.86917897682753|\n",
            "|      Fire-2nd Alarm|    447 Wales Avenue|    Bronx|07/26/2017 03:06:...| 40.81028097169481|-73.90820617557257|\n",
            "|      Fire-7th Alarm|110-14 Liberty Av...|   Queens|03/04/2017 11:13:...| 40.71400364095638|-73.82998933154158|\n",
            "|       Utility-Other|10 Nathan Perlman...|Manhattan|05/02/2019 11:30:...|40.733605005828394|-73.98293627906979|\n",
            "|      Fire-6th Alarm|     702 44th street| Brooklyn|04/03/2019 05:10:...| 40.64567498528483| -74.0022560985342|\n",
            "|        Fire-Haz Mat|     41-12 12 Street|   Queens|06/12/2017 01:36:...| 40.71400364095638|-73.82998933154158|\n",
            "|      Fire-7th Alarm|110-14 Liberty Av...|   Queens|03/04/2017 11:13:...| 40.71400364095638|-73.82998933154158|\n",
            "|      Fire-5th Alarm|2118 Flatbush Avenue| Brooklyn|04/17/2019 06:47:...| 40.61825505522539|-73.93215402943036|\n",
            "|Structural-Street...|FDR Drive & East ...|Manhattan|09/28/2017 07:44:...| 40.78499400000001|-73.94091300000001|\n",
            "|        Fire-Manhole| 651 West 168 Street|Manhattan|11/27/2017 04:53:...|              null|              null|\n",
            "|      Fire-4th Alarm|       1686 clay ave|    Bronx|12/17/2017 05:31:...| 40.84387029225576|-73.90486685899367|\n",
            "|      Fire-6th Alarm|225 McClellan Street|    Bronx|08/05/2019 05:48:...| 40.83227874216628|-73.91658338106646|\n",
            "|  Utility-Water Main|OsbornStreet & Pi...| Brooklyn|09/02/2019 01:42:...| 40.66999199999999|          -73.9089|\n",
            "|      Fire-2nd Alarm|    85 Cooper Street| Brooklyn|10/07/2019 01:25:...| 40.68589980850017|-73.90923962629114|\n",
            "|      Fire-3rd Alarm|667 Jefferson Avenue| Brooklyn|03/05/2018 12:49:...|40.685477405505175|-73.93117632001268|\n",
            "|  Utility-Water Main|    136-17 72 Avenue|   Queens|01/16/2017 01:13:...| 40.71400364095638|-73.82998933154158|\n",
            "|Structural-Street...|FDR Drive & East ...|Manhattan|09/28/2017 07:44:...| 40.78499400000001|-73.94091300000001|\n",
            "|          Fire-Other|         215 8th ave|Manhattan|05/10/2018 07:43:...|40.743937812106836|-73.99950702735288|\n",
            "+--------------------+--------------------+---------+--------------------+------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RENAME COLUMNS which has \"spaces\" in COLUMN Names"
      ],
      "metadata": {
        "id": "6iUoK3yxEQp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper, col\n",
        "import pyspark.sql.functions as f\n",
        "df_spark = df_spark.withColumnRenamed(\"Incident Type\",\"Incident_Type\")\n",
        "df_spark = df_spark.withColumnRenamed(\"Creation Date\",\"Creation_Date\")\n",
        "df_spark.tail(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5doP8P06EBOk",
        "outputId": "f7781415-70a6-419c-e692-8097f3c474be"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Incident_Type='Fire-2nd Alarm', Location='549 Beach 68 Street', Borough='Far Rockaway', Creation_Date='10/29/2016 12:20:18 AM', Latitude=40.74862862455331, Longitude=-73.9856861527595),\n",
              " Row(Incident_Type='HazMat-Other', Location='East 70 Street & York Avenue', Borough='Manhattan', Creation_Date='11/03/2016 04:25:39 PM', Latitude=None, Longitude=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Converting datetime format to Date and Time\n",
        "# Make sure to run this only once"
      ],
      "metadata": {
        "id": "jUXIBab-G4iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession \n",
        "from pyspark.sql.functions import col, substring\n",
        "df_spark = df_spark.select('*',substring(df_spark[3], 1, 10).alias('CREATE_DATE'))\n",
        "df_spark = df_spark.select('*',substring(df_spark[3], 12, 8).alias('CREATE_TIME'))"
      ],
      "metadata": {
        "id": "1ejyEUNqEZug"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark = df_spark.drop('Creation_Date')\n",
        "df_spark = df_spark.drop('Location') #as we have not used it in original dataset"
      ],
      "metadata": {
        "id": "NbLfPPlIGCcf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIik5EANOB4t",
        "outputId": "0987824d-2951-4286-aba4-46da6a2ae1ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------+------------------+------------------+-----------+-----------+\n",
            "|       Incident_Type|  Borough|          Latitude|         Longitude|CREATE_DATE|CREATE_TIME|\n",
            "+--------------------+---------+------------------+------------------+-----------+-----------+\n",
            "|      Fire-2nd Alarm|Manhattan| 40.71442154062271|-74.00607638041981| 11/25/2016|   04:06:09|\n",
            "|Administration-Other|   Queens| 40.71400364095638|-73.82998933154158| 05/10/2017|   01:36:12|\n",
            "|Utility-Gas Low P...|    Bronx| 40.82730091310776|-73.86917897682753| 01/30/2017|   09:47:09|\n",
            "|      Fire-2nd Alarm|    Bronx| 40.81028097169481|-73.90820617557257| 07/26/2017|   03:06:44|\n",
            "|      Fire-7th Alarm|   Queens| 40.71400364095638|-73.82998933154158| 03/04/2017|   11:13:52|\n",
            "|       Utility-Other|Manhattan|40.733605005828394|-73.98293627906979| 05/02/2019|   11:30:49|\n",
            "|      Fire-6th Alarm| Brooklyn| 40.64567498528483| -74.0022560985342| 04/03/2019|   05:10:29|\n",
            "|        Fire-Haz Mat|   Queens| 40.71400364095638|-73.82998933154158| 06/12/2017|   01:36:36|\n",
            "|      Fire-7th Alarm|   Queens| 40.71400364095638|-73.82998933154158| 03/04/2017|   11:13:52|\n",
            "|      Fire-5th Alarm| Brooklyn| 40.61825505522539|-73.93215402943036| 04/17/2019|   06:47:50|\n",
            "|Structural-Street...|Manhattan| 40.78499400000001|-73.94091300000001| 09/28/2017|   07:44:57|\n",
            "|        Fire-Manhole|Manhattan|              null|              null| 11/27/2017|   04:53:34|\n",
            "|      Fire-4th Alarm|    Bronx| 40.84387029225576|-73.90486685899367| 12/17/2017|   05:31:57|\n",
            "|      Fire-6th Alarm|    Bronx| 40.83227874216628|-73.91658338106646| 08/05/2019|   05:48:49|\n",
            "|  Utility-Water Main| Brooklyn| 40.66999199999999|          -73.9089| 09/02/2019|   01:42:36|\n",
            "|      Fire-2nd Alarm| Brooklyn| 40.68589980850017|-73.90923962629114| 10/07/2019|   01:25:44|\n",
            "|      Fire-3rd Alarm| Brooklyn|40.685477405505175|-73.93117632001268| 03/05/2018|   12:49:27|\n",
            "|  Utility-Water Main|   Queens| 40.71400364095638|-73.82998933154158| 01/16/2017|   01:13:38|\n",
            "|Structural-Street...|Manhattan| 40.78499400000001|-73.94091300000001| 09/28/2017|   07:44:57|\n",
            "|          Fire-Other|Manhattan|40.743937812106836|-73.99950702735288| 05/10/2018|   07:43:54|\n",
            "+--------------------+---------+------------------+------------------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Lets pass the dataset through the preprocessing pipeline"
      ],
      "metadata": {
        "id": "5Ey6YlZ1fdDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp=df_spark.rdd"
      ],
      "metadata": {
        "id": "AFEqyoG1fZSD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Date and Time"
      ],
      "metadata": {
        "id": "PCwMqIgNfjOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From data profiling we found that the date and time has no NULL Values"
      ],
      "metadata": {
        "id": "Cci1-UVX9aVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking date and time for null values\n",
        "\n",
        "df_temp_=df_temp.map(lambda x:(x, valid_date_check(x[-2], 'mm/dd/yyyy'))).filter(lambda x: x[1]==True)\n",
        "df_temp=df_temp_.map(lambda x: x[0])"
      ],
      "metadata": {
        "id": "eBrU_gyCfgG1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_=df_temp.map(lambda x:(x, valid_time_check(str(x[-1])))).filter(lambda x: x[1]==True)\n",
        "df_temp=df_temp_.map(lambda x: x[0])"
      ],
      "metadata": {
        "id": "U9o6JgmNsPXQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Geocoding"
      ],
      "metadata": {
        "id": "k0CryG30f201"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#geospacial attributes imputation\n",
        "\n",
        "df_temp=df_temp.toDF(schema=df_spark.schema)\n",
        "df_spk=reverse_geo_code_boros(df_temp, 'Latitude', 'Longitude', 'Borough', -4, -3)"
      ],
      "metadata": {
        "id": "UCaVenmIfn6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cff77fa-efe8-4ff7-d357-3731b486e7a5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "___intializing Zip Code Look up ____\n",
            "____ imputing the points ____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets profile the data now."
      ],
      "metadata": {
        "id": "-K1Yk9XSgcyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandasDF = df_spk.toPandas()\n",
        "ds=stream(pandasDF)\n",
        "\n",
        "#Creating profile of our dataset\n",
        "profiles = ds.profile(default_profiler=DefaultColumnProfiler)\n",
        "profiles.stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "Xnr8RCLtgZZg",
        "outputId": "2959c47f-5164-4b35-c8d3-a56527fc1b29"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total</th>\n",
              "      <th>empty</th>\n",
              "      <th>distinct</th>\n",
              "      <th>uniqueness</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Incident_Type</th>\n",
              "      <td>273</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>0.300366</td>\n",
              "      <td>5.549748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Borough</th>\n",
              "      <td>273</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>2.460617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latitude</th>\n",
              "      <td>273</td>\n",
              "      <td>0</td>\n",
              "      <td>252</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>7.859246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Longitude</th>\n",
              "      <td>273</td>\n",
              "      <td>0</td>\n",
              "      <td>252</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>7.859246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CREATE_DATE</th>\n",
              "      <td>273</td>\n",
              "      <td>0</td>\n",
              "      <td>251</td>\n",
              "      <td>0.919414</td>\n",
              "      <td>7.912833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CREATE_TIME</th>\n",
              "      <td>273</td>\n",
              "      <td>0</td>\n",
              "      <td>263</td>\n",
              "      <td>0.963370</td>\n",
              "      <td>8.006275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               total  empty  distinct  uniqueness   entropy\n",
              "Incident_Type    273      0        82    0.300366  5.549748\n",
              "Borough          273      0        12    0.043956  2.460617\n",
              "Latitude         273      0       252    0.923077  7.859246\n",
              "Longitude        273      0       252    0.923077  7.859246\n",
              "CREATE_DATE      273      0       251    0.919414  7.912833\n",
              "CREATE_TIME      273      0       263    0.963370  8.006275"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Precision and Recall\n",
        "\n",
        "Selected data[Borough]  =351\n",
        "\n",
        "Selected data[Lat]\t= 351\n",
        "\n",
        "Selected data[Lon]\t= 351\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "True positive[borough] = 351 - 0 -78 = 273\n",
        "\n",
        "True positive[lat]\t= 351 -53 - 78 = 220\n",
        "\n",
        "True positive[lat] \t= 351 - 53 - 78= 220\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Relevant [borough] = 273 - 0 = 273\n",
        "\n",
        "Relevant[latitude] = 273 - 53 = 220\n",
        "\n",
        "Relevant[longitude] = 273 - 53 = 220\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "We are calculating precision as \n",
        "\n",
        "precision = True positive/selected\n",
        "\n",
        "recall = True positive/Relavent\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Precision[borough]:    87%\n",
        "\n",
        "Precision[latitude]:  75%\n",
        "\n",
        "Precision[longitude]:  75%\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Recall[borough]:  99%\n",
        "\n",
        "Recall[latitude]: 98%\n",
        "\n",
        "Recall[longitude]:  99%\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_qoQzaATgHr0"
      }
    }
  ]
}