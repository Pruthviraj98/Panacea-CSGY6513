{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rodent_Inspection_Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Rodent Inspection:\n",
        "\n",
        "a. To get the required columns, use this module: \n",
        "\n",
        "\n",
        "1.   get_area_of_interest(df_spark, interested_columns)\n",
        "\n",
        "\n",
        "b. Preprocessing pipeline: Pass your data through these functions. (if your columns fall in those categories)\n",
        "\n",
        "1.   valid_date_check(date)\n",
        "2.   reverse_geo_code_boros(df_spark, Latitude, Longitude, Boro, lat_index, long_index)\n",
        "3.   valid_borough_check(borough)\n",
        "4.   to_check_long(longitude)\n",
        "5.   to_check_lat(latitude)"
      ],
      "metadata": {
        "id": "EtwLWOuvVE1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8dUqIINUs-T",
        "outputId": "0073490f-5f61-4368-bd23-3ba8bc2f6bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 39 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 44.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=acc6af7c6bd3cd7feaad6c0ffa74e20041c9a1fc1b127ea8640cad7f833289ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
            "Collecting openclean\n",
            "  Downloading openclean-0.2.1-py3-none-any.whl (5.2 kB)\n",
            "Collecting openclean-core==0.4.1\n",
            "  Downloading openclean_core-0.4.1-py3-none-any.whl (267 kB)\n",
            "\u001b[K     |████████████████████████████████| 267 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (1.1.5)\n",
            "Collecting jsonschema>=3.2.0\n",
            "  Downloading jsonschema-4.2.1-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting flowserv-core>=0.8.0\n",
            "  Downloading flowserv_core-0.9.2-py3-none-any.whl (260 kB)\n",
            "\u001b[K     |████████████████████████████████| 260 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting jellyfish\n",
            "  Downloading jellyfish-0.8.9.tar.gz (137 kB)\n",
            "\u001b[K     |████████████████████████████████| 137 kB 65.6 MB/s \n",
            "\u001b[?25hCollecting refdata>=0.2.0\n",
            "  Downloading refdata-0.2.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from openclean-core==0.4.1->openclean) (0.3.4)\n",
            "Collecting histore>=0.4.0\n",
            "  Downloading histore-0.4.1-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy>=1.3.18 in /usr/local/lib/python3.7/dist-packages (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (1.4.27)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click in /usr/local/lib/python3.7/dist-packages (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (7.1.2)\n",
            "Collecting passlib\n",
            "  Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
            "\u001b[K     |████████████████████████████████| 525 kB 49.0 MB/s \n",
            "\u001b[?25hCollecting paramiko\n",
            "  Downloading paramiko-2.8.1-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 61.3 MB/s \n",
            "\u001b[?25hCollecting gitpython\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 50.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml-include\n",
            "  Downloading pyyaml_include-1.2.post2-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from histore>=0.4.0->openclean-core==0.4.1->openclean) (5.4.8)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (4.8.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (0.18.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (21.2.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->openclean-core==0.4.1->openclean) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->openclean-core==0.4.1->openclean) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->openclean-core==0.4.1->openclean) (1.15.0)\n",
            "Collecting tableprint\n",
            "  Downloading tableprint-0.9.1-py3-none-any.whl (6.8 kB)\n",
            "Collecting datasize>=1.0.0\n",
            "  Downloading datasize-1.0.0.tar.gz (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pooch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from refdata>=0.2.0->openclean-core==0.4.1->openclean) (1.5.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.3.0->refdata>=0.2.0->openclean-core==0.4.1->openclean) (21.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy>=1.3.18->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.3.0->refdata>=0.2.0->openclean-core==0.4.1->openclean) (3.0.6)\n",
            "Collecting cryptography>=2.5\n",
            "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 42.2 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
            "\u001b[K     |████████████████████████████████| 961 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (2.21)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->openclean-core==0.4.1->openclean) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->openclean-core==0.4.1->openclean) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->openclean-core==0.4.1->openclean) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->openclean-core==0.4.1->openclean) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->openclean-core==0.4.1->openclean) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->openclean-core==0.4.1->openclean) (1.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from tableprint->refdata>=0.2.0->openclean-core==0.4.1->openclean) (0.2.5)\n",
            "Building wheels for collected packages: datasize, jellyfish\n",
            "  Building wheel for datasize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datasize: filename=datasize-1.0.0-py2.py3-none-any.whl size=155047 sha256=35e5741dddce7aba8521d1a060fb39c4655a584e932f8be79055a113136a0885\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/b5/32/d8836896da6aca7f9c5748670ea6110d1385c262bf3abcca30\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.8.9-cp37-cp37m-linux_x86_64.whl size=73234 sha256=35f722ef78fc093e38cb4a81c80dce9510c682b99b2673d19644bd5ad4f0f621\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/a9/ef/5d8742e72deaf0d1de327a180d008c2c0299367581800ea73f\n",
            "Successfully built datasize jellyfish\n",
            "Installing collected packages: smmap, pyyaml, pynacl, gitdb, cryptography, bcrypt, tableprint, pyyaml-include, passlib, paramiko, jsonschema, gitpython, datasize, refdata, jellyfish, histore, flowserv-core, openclean-core, openclean\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.5.9 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed bcrypt-3.2.0 cryptography-36.0.0 datasize-1.0.0 flowserv-core-0.9.2 gitdb-4.0.9 gitpython-3.1.24 histore-0.4.1 jellyfish-0.8.9 jsonschema-4.2.1 openclean-0.2.1 openclean-core-0.4.1 paramiko-2.8.1 passlib-1.7.4 pynacl-1.4.0 pyyaml-5.4.1 pyyaml-include-1.2.post2 refdata-0.2.0 smmap-5.0.0 tableprint-0.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install openclean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing packages required\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import os\n",
        "import requests\n",
        "from six.moves import urllib\n",
        "import sys \n",
        "import pandas as pd\n",
        "import matplotlib \n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import IPython\n",
        "from IPython import display\n",
        "import sklearn\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from openclean.pipeline import stream\n",
        "from openclean.profiling.column import DefaultColumnProfiler\n",
        "from openclean.data.source.socrata import Socrata\n",
        "from openclean.pipeline import stream\n",
        "from openclean.function.eval.datatype import IsDatetime\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import udf, struct\n",
        "from pyspark.sql.types import StringType"
      ],
      "metadata": {
        "id": "fOq89ZjQU0yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import ArcGIS\n",
        "geocoder=ArcGIS()\n",
        "#example:\n",
        "geocoder.reverse('40.61157006600007, -73.74736517199995')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjam1thSU34J",
        "outputId": "6f4af6be-ed50-48ae-82ae-0333bf8621c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Location(11-64 Redfern Ave, Far Rockaway, New York 11691, USA, (40.61161616586613, -73.74738361194636, 0.0))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Spark Session\n",
        "sc = SparkContext.getOrCreate();\n",
        "spark = SparkSession(sc)"
      ],
      "metadata": {
        "id": "Jye4yIQdU7WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading file from NYC Open Data\n",
        "fn_src = 'https://data.cityofnewyork.us/api/views/p937-wjvj/rows.csv?accessType=DOWNLOAD'\n",
        "fn_dst = '/content/Rodent_Inspection.csv'\n",
        "\n",
        "from six.moves import urllib\n",
        "\n",
        "if os.path.isfile(fn_dst):\n",
        "    print('File %s has already been downloaded' % fn_dst)\n",
        "else:\n",
        "    urllib.request.urlretrieve(fn_src, fn_dst)\n",
        "    print('File %s has been downloaded' % fn_dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkeHipz9U-Rj",
        "outputId": "6ffee30c-a5d5-42e0-f9cf-700bcbe2b2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/Rodent_Inspection.csv has been downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = 'https://data.beta.nyc/dataset/0ff93d2d-90ba-457c-9f7e-39e47bf2ac5f/resource/7caac650-d082-4aea-9f9b-3681d568e8a5/download/nyc_zip_borough_neighborhoods_pop.csv'\n",
        "dst = '/content/nyc_zip_borough_neighborhoods_pop.csv'\n",
        "\n",
        "#https://data.cityofnewyork.us/resource/h9gi-nx95.csv\n",
        "\n",
        "from six.moves import urllib\n",
        "\n",
        "if os.path.isfile(dst):\n",
        "    print('File %s has already been downloaded' % dst)\n",
        "else:\n",
        "    urllib.request.urlretrieve(src, dst)\n",
        "    print('File %s has been downloaded' % dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkp1LzrKWM99",
        "outputId": "ef5a2552-863b-4ee6-b8a6-5cf11d16cfd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/nyc_zip_borough_neighborhoods_pop.csv has been downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#similarly, lets get them into pyspark rdd\n",
        "def get_area_of_interest(df_spark, interested_columns):\n",
        "  df_spark=df_spark.select(interested_columns)\n",
        "  return df_spark"
      ],
      "metadata": {
        "id": "QSORcsBrVfcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Module for date related columns\n",
        "\n",
        "As the dataset is for the data from 2006 to 2020, we can see that there is data from unknown format of \"1010-05-14\" to the year 2020. We need to clean this. Over here, we remove the null values where the complaint date is <2006. "
      ],
      "metadata": {
        "id": "up2fhHPMhWXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fileName='1010-05-14 00:00:00'\n",
        "# # matches=re.search(\"([0-9]{4}\\-[0-9]{2}\\-[0-9]{2})\", fileName)\n",
        "# re.search(r'([0-9]{4}\\-[0-9]{2}\\-[0-9]{2})', fileName).group(0)\n",
        "\n",
        "def valid_date_check(date):\n",
        "  if date==None or date==\" \" or date==\"\":\n",
        "      return False\n",
        "  else:\n",
        "    date,time,type = date.split(\" \")\n",
        "    date_cpy=date\n",
        "    date=date.split(\"/\")\n",
        "    try:\n",
        "      month=int(date[0])\n",
        "      day= int(date[1])\n",
        "      year=int(date[2])\n",
        "      if year>=2006 and year<=2020:\n",
        "        try:\n",
        "          refined_date=datetime.datetime(year, month, day)\n",
        "          return True\n",
        "        except:\n",
        "          return False\n",
        "      else:\n",
        "        return False\n",
        "    except:\n",
        "      return False"
      ],
      "metadata": {
        "id": "kml5KmPEeAK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.b Module for Reverse Geocoding the boroughs using latitudes and longitudes.\n",
        "\n",
        "1. First we will remove the rows where latitude, longitude and boroughs are null. (around 450 tuples removed)\n",
        "2. Then, where the boroughs are empty, take the latitude and longitude value and reverse geocode it using the module \"reverseGeocoder\".\n",
        "3. Impute the borough name retrived in the empty space.\n",
        "\n",
        "\n",
        "### USING MASTER DATASET\n",
        "In the case of geocoding, geocoder gives us the zipcodes based on the latitude and longitude values. Inturn, we can use the master dataset of zipcodes inorder to retrive the borough names\n",
        "\n",
        "\n",
        "\n",
        "NOTE: The dataset can be downloaded from : https://data.beta.nyc/en/dataset/pediacities-nyc-neighborhoods/resource/7caac650-d082-4aea-9f9b-3681d568e8a5"
      ],
      "metadata": {
        "id": "YtxVasX_g8OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reverseGeoCoder(latitude, longitude):\n",
        "  loc=geocoder.reverse(str(latitude)+', '+str(longitude), timeout=10)\n",
        "  zipCode=str(loc).split(\",\")[2][-5:]\n",
        "  if not int(zipCode) in zip_master:\n",
        "    boro=\"UNKNOWN\"\n",
        "  else:\n",
        "    boro=zip_master[int(zipCode)]\n",
        "  boro=boro.upper()\n",
        "  return boro\n",
        "\n",
        "def reverse_geo_code_boros(df_spark, Latitude, Longitude, Boro, lat_index, long_index, master_path):\n",
        "  #select data where we have to impute\n",
        "  df_temp_boro_clean=df_spark.filter((df_spark[Latitude].isNotNull()) & (df_spark[Longitude].isNotNull()))\n",
        "  boro_cleaner=df_temp_boro_clean.filter((df_temp_boro_clean[Boro].isNull())|(df_temp_boro_clean[Boro]=='NEW YORK'))\n",
        "  print(\"We have \"+ str(boro_cleaner.count())+ \" points to impute\")\n",
        "  print(\"___intializing Zip Code Look up ____\")\n",
        "  \n",
        "  #use your path for master dataset here. \n",
        "  df_zips=pd.read_csv(master_path)\n",
        "  zip_master={}\n",
        "  zips=df_zips['zip']\n",
        "  boro=df_zips['borough']\n",
        "  for i, j in zip(zips, boro):\n",
        "    zip_master[i]=j\n",
        "  zip_master[10020]='Manhattan'\n",
        "  zip_master[11249]='Brooklyn'\n",
        "\n",
        "  print(\"____ imputing the points ____\")\n",
        "  #creating UD function\n",
        "  ud_func= udf(reverseGeoCoder, StringType())\n",
        "  boro_cleaned_dataframe = boro_cleaner.withColumn(Boro, ud_func(boro_cleaner[lat_index], boro_cleaner[long_index]))\n",
        "\n",
        "  #joining the imputed dataset to the maindataset and returning\n",
        "  joiner_dataset=df_spark.filter((df_spark[Latitude].isNotNull()) & (df_spark[Longitude].isNotNull()) & (df_spark[Boro].isNotNull()))\n",
        "  fin_df=joiner_dataset.union(boro_cleaned_dataframe)\n",
        "  return fin_df"
      ],
      "metadata": {
        "id": "tGGvupG0eQsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of dataset ~ 24k tuples. So, we need around 2000 data points for 95% confidence level with 2% interval. The size of data is almost 10% of the data. So we can get it into our df now"
      ],
      "metadata": {
        "id": "aWqXQ6Xmgz70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark=spark.read.option(\"header\",True).csv(fn_dst,inferSchema=True)\n",
        "df_spark=df_spark.sample(0.001)\n",
        "df_spark.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq0_3ZPweYnA",
        "outputId": "f71e5c1e-1d50-4a34-b79e-3fc1dc2b5612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2041"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7oQ64i0LrZO",
        "outputId": "dae514bd-99ac-477c-bcaa-90f67f723bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- INSPECTION_TYPE: string (nullable = true)\n",
            " |-- JOB_TICKET_OR_WORK_ORDER_ID: integer (nullable = true)\n",
            " |-- JOB_ID: string (nullable = true)\n",
            " |-- JOB_PROGRESS: integer (nullable = true)\n",
            " |-- BBL: long (nullable = true)\n",
            " |-- BORO_CODE: integer (nullable = true)\n",
            " |-- BLOCK: integer (nullable = true)\n",
            " |-- LOT: integer (nullable = true)\n",
            " |-- HOUSE_NUMBER: string (nullable = true)\n",
            " |-- STREET_NAME: string (nullable = true)\n",
            " |-- ZIP_CODE: integer (nullable = true)\n",
            " |-- X_COORD: integer (nullable = true)\n",
            " |-- Y_COORD: integer (nullable = true)\n",
            " |-- LATITUDE: double (nullable = true)\n",
            " |-- LONGITUDE: double (nullable = true)\n",
            " |-- BOROUGH: string (nullable = true)\n",
            " |-- INSPECTION_DATE: string (nullable = true)\n",
            " |-- RESULT: string (nullable = true)\n",
            " |-- APPROVED_DATE: string (nullable = true)\n",
            " |-- LOCATION: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Select the columns that are common with the original dataset:\n",
        "1. BOROUGH\n",
        "2. Latitude\n",
        "3. Longitude\n",
        "4. Inspection_Date\n",
        "\n",
        "We can consider the primary key along with this\n",
        "5. INSPECTION_TYPE\n",
        "6. JOB_TICKET_OR_WORK_ORDER_ID"
      ],
      "metadata": {
        "id": "paHwx8COfUAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interested_columns_1=['INSPECTION_TYPE', 'JOB_TICKET_OR_WORK_ORDER_ID', 'INSPECTION_DATE', 'BOROUGH', 'LATITUDE', 'LONGITUDE']\n",
        "df_spark=get_area_of_interest(df_spark, interested_columns_1)"
      ],
      "metadata": {
        "id": "GymGzo9SfMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLHP0ap7NJJ0",
        "outputId": "8a83002c-ffb5-482f-a266-56da0ad57204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2041"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp=df_spark.rdd"
      ],
      "metadata": {
        "id": "AFEqyoG1fZSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBAzSlYAOQCD",
        "outputId": "759be8c2-b757-4ab3-e38a-a2e2b5b47938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(INSPECTION_TYPE='Initial', JOB_TICKET_OR_WORK_ORDER_ID=13282421, INSPECTION_DATE='08/27/2021 08:52:43 AM', BOROUGH='Queens', LATITUDE=40.70621577643, LONGITUDE=-73.911020313495),\n",
              " Row(INSPECTION_TYPE='Initial', JOB_TICKET_OR_WORK_ORDER_ID=13282992, INSPECTION_DATE='08/27/2021 09:55:00 AM', BOROUGH='Staten Island', LATITUDE=40.634265775352, LONGITUDE=-74.100311764687)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Date and Time"
      ],
      "metadata": {
        "id": "PCwMqIgNfjOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_=df_temp.map(lambda x:(x, valid_date_check(x[2]))).filter(lambda x: x[1]==True)\n",
        "df_temp=df_temp_.map(lambda x: x[0])"
      ],
      "metadata": {
        "id": "eBrU_gyCfgG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSytJrdsPFGr",
        "outputId": "c600d989-8d15-4e51-d28f-2cfe130bea3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1941"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #as this code requires the pyspark dataframe(Not the rdd)\n",
        "df_temp=df_temp.toDF(schema=df_spark.schema)"
      ],
      "metadata": {
        "id": "UCaVenmIfn6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Geocoding"
      ],
      "metadata": {
        "id": "k0CryG30f201"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spk=reverse_geo_code_boros(df_temp, 'LATITUDE', 'LONGITUDE', 'BOROUGH', -2, -1, dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_PzQ4oaf2OH",
        "outputId": "d8bb7f07-3b40-40ff-854b-b18a6c07fe3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 0 points to impute\n",
            "___intializing Zip Code Look up ____\n",
            "____ imputing the points ____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets profile the data now."
      ],
      "metadata": {
        "id": "-K1Yk9XSgcyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandasDF = df_spk.toPandas()\n",
        "ds=stream(pandasDF)\n",
        "\n",
        "#Creating profile of our dataset\n",
        "profiles = ds.profile(default_profiler=DefaultColumnProfiler)\n",
        "profiles.stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Xnr8RCLtgZZg",
        "outputId": "95255790-ae18-4318-a978-f27e38638987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total</th>\n",
              "      <th>empty</th>\n",
              "      <th>distinct</th>\n",
              "      <th>uniqueness</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>INSPECTION_TYPE</th>\n",
              "      <td>1938</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.001548</td>\n",
              "      <td>1.139824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JOB_TICKET_OR_WORK_ORDER_ID</th>\n",
              "      <td>1938</td>\n",
              "      <td>0</td>\n",
              "      <td>1938</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.920353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>INSPECTION_DATE</th>\n",
              "      <td>1938</td>\n",
              "      <td>0</td>\n",
              "      <td>1938</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.920353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BOROUGH</th>\n",
              "      <td>1938</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0.002580</td>\n",
              "      <td>1.987243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LATITUDE</th>\n",
              "      <td>1938</td>\n",
              "      <td>0</td>\n",
              "      <td>1897</td>\n",
              "      <td>0.978844</td>\n",
              "      <td>10.864434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LONGITUDE</th>\n",
              "      <td>1938</td>\n",
              "      <td>0</td>\n",
              "      <td>1897</td>\n",
              "      <td>0.978844</td>\n",
              "      <td>10.864434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             total  empty  distinct  uniqueness    entropy\n",
              "INSPECTION_TYPE               1938      0         3    0.001548   1.139824\n",
              "JOB_TICKET_OR_WORK_ORDER_ID   1938      0      1938    1.000000  10.920353\n",
              "INSPECTION_DATE               1938      0      1938    1.000000  10.920353\n",
              "BOROUGH                       1938      0         5    0.002580   1.987243\n",
              "LATITUDE                      1938      0      1897    0.978844  10.864434\n",
              "LONGITUDE                     1938      0      1897    0.978844  10.864434"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Precision And Recall:\n",
        "\n",
        "Here the reason behind lower precision is the inspection date which can be of any time period, but our original dataset restricts it between 2006 and 2020\n",
        "\n",
        "True Positive = 1941\n",
        "selected elements = 2041\n",
        "Relevant elements = 1941\n",
        "\n",
        "precision= 1941/2041\n",
        "recall = 1941/2041\n",
        " "
      ],
      "metadata": {
        "id": "yOu5bqbfQoT-"
      }
    }
  ]
}